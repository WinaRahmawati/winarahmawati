#import modul
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from time import time
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn_extensions.extreme_learning_machines.elm import ELMClassifier
import warnings; warnings.simplefilter('ignore')

#load dataset
dataset = pd.read_excel("name_dataset.xlsx")
dataset.shape
dataset.head()
a = dataset.describe()
a.to_excel('describe_dataset.xlsx')

#Separating Feature and Label
label = dataset['Y']
fitur = dataset.drop('Y', axis=1)
X = fitur.values
y = label.values

#Correlation Value With Heatmap
sns.set(style="white")
corr = fitur.corr()
mask = np.zeros_like(corr, dtype=bool)
mask[np.triu_indices_from(mask)] = True
f, ax = plt.subplots(figsize=(11,9))
cmap = sns.diverging_palette(220,10,as_cmap=False)
sns.heatmap(corr,mask=mask,cmap=cmap,square=True, ax=ax)
ax.set_title('Multi-Collinearity of Features')
plt.show()

#Data Distribution
Xax=X_pca[:,0]
Yax=X_pca[:,1]

cdict={0:'red',1:'green'}
labl={0:'Tidak Berinteraksi',1:'Berinteraksi'}
marker={0:'*',1:'o'}
alpha={0:.3, 1:.5}
fig,ax=plt.subplots(figsize=(7,5))
fig.patch.set_facecolor('white')

for l in np.unique(label):
 ix=np.where(label==l)
 ax.scatter(Xax[ix],Yax[ix],c=cdict[l],s=40,
           label=labl[l],marker=marker[l],alpha=alpha[l])
# for loop ends
plt.xlabel("First Principal Component",fontsize=14)
plt.ylabel("Second Principal Component",fontsize=14)
plt.legend()
plt.show()

#Train and Test Splits
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

#Function to Build Model
def grid_search_wrapper(param_grid, scoring, X_train, X_test, y_train, y_test, refit_score='accuracy_score'):
    elmc = ELMClassifier(rbf_width=0.0, alpha=1.0, random_state=0)
    search = GridSearchCV(elmc, param_grid, cv = 10, scoring=scorers, refit=refit_score, \
                          n_jobs=-1, return_train_score=True)
    search.fit(X_train, y_train)
    
    hasil_gridsearch = {
            'rangkuman':pd.DataFrame(search.cv_results_),
            'best_estimator': search.best_estimator_,
            'best_score_': search.best_score_,
            'best_parameters': search.best_params_}
    
    y_pred = search.predict(X_test)
    
     # confusion matrix on the test data.
    conf_mat = pd.DataFrame(confusion_matrix(y_test, y_pred),
                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos'])
    target_names = ['Berinteraksi', 'Tidak Berinteraksi']
    class_report_test = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)
    class_report_test = pd.DataFrame(class_report_test).transpose()
    
    hasil_test = {
            'confusion_matrix': conf_mat,
            'class_report_test':class_report_test}
    return {'hasil_gridsearch': hasil_gridsearch , 'hasil_test': hasil_test}

#Train Model
param_grid = {'n_hidden' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], 
              'activation_func' : ['sine', 'tanh', 'tribas', 'inv_tribas', 'sigmoid', 'hardlim', 'softlim', 'gaussian', 'multiquadric', 'inv_multiquadric']
             }

scorers = {
    'precision_score': make_scorer(precision_score),
    'recall_score': make_scorer(recall_score),
    'spesificity_score': make_scorer(specificity_score),
    'accuracy_score': make_scorer(accuracy_score),
    'f1_score': make_scorer(f1_score)
}


grid_search_elmc = grid_search_wrapper(param_grid = param_grid, scoring = scorers, X_train = X_train, 
                                       X_test = X_test, 
                                       y_train = y_train, 
                                       y_test = y_test, 
                                       refit_score='accuracy_score')

#save hasil grid search di excel file
rangkuman_gridscv = grid_search_elmc['hasil_gridsearch']['rangkuman'].to_excel('rangkuman_gridscvL3r.xlsx')
best_estimator_gridscv = grid_search_elmc['hasil_gridsearch']['best_estimator']
best_score_gridscv = grid_search_elmc['hasil_gridsearch']['best_score_'] #average of accuracy value of 10 fold 
best_parameters_gridscv = grid_search_elmc['hasil_gridsearch']['best_parameters']

print({'best_estimator_gridscv':best_estimator_gridscv,
       'best_score_gridscv': best_score_gridscv,
       'best_parameters_gridscv': best_parameters_gridscv})

report_hasil_testing = grid_search_elmc['hasil_test']['class_report_test'].to_excel('report_hasil_testingL3r.xlsx')

#save model
import pickle

f = open('model_L3r.pckl', 'wb')
pickle.dump(grid_search_elmc,f)
f.close()
